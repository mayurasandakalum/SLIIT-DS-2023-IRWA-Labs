{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 02"
      ],
      "metadata": {
        "id": "_XaNEumKRpp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# mount the gdrive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dPxYS4wTQdJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87388c24-7f18-4852-f23b-e078463faee8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question 01***"
      ],
      "metadata": {
        "id": "eLUcs2JLRe69"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DJ8S8rTyHygR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b793c10b-7ece-430c-daaf-fe07f3274733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IRWA/inverted\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'breakthrough': ['Doc1.txt'],\n",
              " 'drug': ['Doc1.txt', 'Doc2.txt'],\n",
              " 'for': ['Doc1.txt', 'Doc3.txt', 'Doc4.txt'],\n",
              " 'schizophrenia': ['Doc1.txt', 'Doc2.txt', 'Doc3.txt', 'Doc4.txt'],\n",
              " 'new': ['Doc2.txt', 'Doc3.txt', 'Doc4.txt'],\n",
              " 'approach': ['Doc3.txt'],\n",
              " 'treatment': ['Doc3.txt'],\n",
              " 'of': ['Doc3.txt'],\n",
              " 'hopes': ['Doc4.txt'],\n",
              " 'patients': ['Doc4.txt']}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# set working dir\n",
        "%cd /content/drive/MyDrive/IRWA/inverted\n",
        "\n",
        "# get current working dir\n",
        "cwd = os.getcwd()\n",
        "\n",
        "# get file names in current dir\n",
        "file_list = os.listdir(cwd)\n",
        "\n",
        "# a)\n",
        "def inverted_index():\n",
        "  # {} === dict()\n",
        "  word_dict = dict()\n",
        "\n",
        "  for file in file_list:\n",
        "    with open(cwd + \"/\" + file, \"r\") as f:\n",
        "      words = f.read().lower().split()\n",
        "      for word in words:\n",
        "        if word not in word_dict:\n",
        "          word_dict[word] = [file]\n",
        "        else:\n",
        "          word_dict[word] += [file]\n",
        "\n",
        "  return word_dict\n",
        "\n",
        "inverted_index()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practice 1 - Set"
      ],
      "metadata": {
        "id": "bj62WYWaWq0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = {1, 2, 3, 4, 5}\n",
        "b = {3, 4, 5, 6, 7}\n",
        "\n",
        "print(\"isDisjoint:\", a.isdisjoint(b))\n",
        "print(\"isSubset (a <= b):\", a.issubset(b)) # a <= b\n",
        "print(\"a < b:\", a < b)\n",
        "print(\"isSuperset (a >= b):\", a.issuperset(b)) # a >= b\n",
        "print(\"a > b:\", a > b)\n",
        "print(\"union:\", a.union(b)) # a | b\n",
        "print(\"intersection:\", a.intersection(b)) # a & b\n",
        "print(\"difference:\", a.difference(b)) # a - b\n",
        "print(\"symmetric_difference:\", a.symmetric_difference(b)) # a ^ b\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPRq3cmQSICs",
        "outputId": "4d77f73f-fc28-4d5a-b0d3-0afa0f90265a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "isDisjoint: False\n",
            "isSubset (a <= b): False\n",
            "a < b: False\n",
            "isSuperset (a >= b): False\n",
            "a > b: False\n",
            "union: {1, 2, 3, 4, 5, 6, 7}\n",
            "intersection: {3, 4, 5}\n",
            "difference: {1, 2}\n",
            "symmetric_difference: {1, 2, 6, 7}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for AND query\n",
        "def and_query(list1, list2):\n",
        "  return set(list1).intersection(set(list2))\n",
        "\n",
        "# function for OR query\n",
        "def or_query(list1, list2):\n",
        "  return set(list1).union(set(list2))"
      ],
      "metadata": {
        "id": "2kib3N0aW-dr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_dict = inverted_index() # return value is a dictionary\n",
        "\n",
        "# (i)\n",
        "for key, doc_list in index_dict.items():\n",
        "  if key == \"schizophrenia\":\n",
        "    schizophrenia_docs_list = doc_list\n",
        "\n",
        "  if key == \"drug\":\n",
        "    drug_docs_list = doc_list\n",
        "\n",
        "print(\"schizophrenia AND drug:\", and_query(schizophrenia_docs_list, drug_docs_list))\n",
        "\n",
        "# (ii)\n",
        "# for"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGhJc3crjfrl",
        "outputId": "a19331dc-58cb-4e36-a820-42479d05ca95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "schizophrenia AND drug: {'Doc2.txt', 'Doc1.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question 02***"
      ],
      "metadata": {
        "id": "ht4HJXpuMAgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punkt Sentence Tokenizer**\n",
        "\n",
        "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Casefold()**\n",
        "\n",
        "The casefold() method returns a string where all the characters are lower case.\n",
        "\n",
        "This method is similar to the lower() method, but the casefold() method is stronger, more aggressive, meaning that it will convert more characters into lower case, and will find more matches when comparing two strings and both are converted using the casefold() method."
      ],
      "metadata": {
        "id": "yMxARwxvfT32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# part A\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize  import  word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "quote = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success\"\n",
        "filtered_list = []\n",
        "\n",
        "words = word_tokenize(quote)\n",
        "\n",
        "# method 1\n",
        "for word in words:\n",
        "  if word.casefold() not in stop_words:\n",
        "    filtered_list.append(word)\n",
        "\n",
        "print(filtered_list)\n",
        "\n",
        "# method 2\n",
        "[word for word in words if word.casefold() not in stop_words]"
      ],
      "metadata": {
        "id": "rcG_MwdIfM9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd56e30-7564-464e-e4a5-f356ac600faa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pythoners', 'intelligent', 'work', 'pythonly', 'pythoning', 'way', 'success']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Pythoners', 'intelligent', 'work', 'pythonly', 'pythoning', 'way', 'success']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding new words to the stop_words\n",
        "new_stop_words = list(stop_words)\n",
        "new_stop_words.extend([\"intelligent\", \"work\"])\n",
        "\n",
        "[word for word in words if word.casefold() not in new_stop_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkoJzj69yBj5",
        "outputId": "fb5fe85e-2ec6-473c-b17e-0359f0c00c58"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Pythoners', 'pythonly', 'pythoning', 'way', 'success']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# part B\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "quote = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success\"\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(quote)\n",
        "print(\"Tokenized words:\", words)\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k6VP-g2EIeO",
        "outputId": "0d5b2935-3535-4cd2-80df-b3452bf8a3ab"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words: ['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success']\n",
            "Stemmed words: ['python', 'are', 'veri', 'intellig', 'and', 'work', 'veri', 'pythonli', 'and', 'now', 'they', 'are', 'python', 'their', 'way', 'to', 'success']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a62o9SdnNUGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Define a function to tokenize and stem a document\n",
        "def tokenize_and_stem(document):\n",
        "    tokens = nltk.word_tokenize(document)\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "    return stems\n",
        "\n",
        "# Define a function to build a positional index\n",
        "def build_positional_index(documents):\n",
        "    positional_index = defaultdict(list)\n",
        "\n",
        "    for doc_id, document in enumerate(documents):\n",
        "        stems = tokenize_and_stem(document)\n",
        "        for position, stem in enumerate(stems):\n",
        "            positional_index[stem].append((doc_id, position))\n",
        "\n",
        "    return positional_index\n",
        "\n",
        "# Example usage with a list of document texts\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"The second document is here.\",\n",
        "    \"And this is the third document.\"\n",
        "]\n",
        "\n",
        "positional_index = build_positional_index(documents)\n",
        "\n",
        "# Print the positional index\n",
        "for term, positions in positional_index.items():\n",
        "    print(f\"{term}: {positions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "974dltRpNUBn",
        "outputId": "de954312-1c2f-4657-eb5d-9239d1bc85fa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thi: [(0, 0), (2, 1)]\n",
            "is: [(0, 1), (1, 3), (2, 2)]\n",
            "the: [(0, 2), (1, 0), (2, 3)]\n",
            "first: [(0, 3)]\n",
            "document: [(0, 4), (1, 2), (2, 5)]\n",
            ".: [(0, 5), (1, 5), (2, 6)]\n",
            "second: [(1, 1)]\n",
            "here: [(1, 4)]\n",
            "and: [(2, 0)]\n",
            "third: [(2, 4)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}