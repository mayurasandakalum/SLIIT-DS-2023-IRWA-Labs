{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "doc_list = os.listdir(cwd + \"/ngram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D1': 'I am Sam',\n",
       " 'D2': 'Sam I am',\n",
       " 'D3': 'I do not like green eggs and ham',\n",
       " 'D4': 'I do not like them, Sam I am'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = {}\n",
    "for doc in doc_list:\n",
    "    with open(cwd + \"/ngram/\" + doc) as f:\n",
    "        docs[doc.split(\".\")[0]] = f.readline()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mssel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D1': ['I', 'am', 'Sam'],\n",
       " 'D2': ['Sam', 'I', 'am'],\n",
       " 'D3': ['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham'],\n",
       " 'D4': ['I', 'do', 'not', 'like', 'them', ',', 'Sam', 'I', 'am']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "after_tokenized = {}\n",
    "\n",
    "for doc, sentence in docs.items():\n",
    "    after_tokenized[doc] = word_tokenize(sentence)\n",
    "\n",
    "after_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Value: 1 -------------------\n",
      "D1: [('I',), ('am',), ('Sam',)]\n",
      "D2: [('Sam',), ('I',), ('am',)]\n",
      "D3: [('I',), ('do',), ('not',), ('like',), ('green',), ('eggs',), ('and',), ('ham',)]\n",
      "D4: [('I',), ('do',), ('not',), ('like',), ('them',), (',',), ('Sam',), ('I',), ('am',)]\n",
      "\n",
      "\n",
      "K-Value: 2 -------------------\n",
      "D1: [('I', 'am'), ('am', 'Sam')]\n",
      "D2: [('Sam', 'I'), ('I', 'am')]\n",
      "D3: [('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'green'), ('green', 'eggs'), ('eggs', 'and'), ('and', 'ham')]\n",
      "D4: [('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'them'), ('them', ','), (',', 'Sam'), ('Sam', 'I'), ('I', 'am')]\n",
      "\n",
      "\n",
      "K-Value: 3 -------------------\n",
      "D1: [('I', 'am', 'Sam')]\n",
      "D2: [('Sam', 'I', 'am')]\n",
      "D3: [('I', 'do', 'not'), ('do', 'not', 'like'), ('not', 'like', 'green'), ('like', 'green', 'eggs'), ('green', 'eggs', 'and'), ('eggs', 'and', 'ham')]\n",
      "D4: [('I', 'do', 'not'), ('do', 'not', 'like'), ('not', 'like', 'them'), ('like', 'them', ','), ('them', ',', 'Sam'), (',', 'Sam', 'I'), ('Sam', 'I', 'am')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def generate_ngrams(sentence, k):\n",
    "    return list(ngrams(sentence, k))\n",
    "\n",
    "k_values = [1, 2, 3]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"K-Value: {k} -------------------\")\n",
    "    for doc, tokenized_sentence in after_tokenized.items():\n",
    "        print(f\"{doc}: {generate_ngrams(tokenized_sentence, k)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coefficient(set1: set, set2: set):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coefficient for k = 1----------\n",
      "D1 and D2: 1.0\n",
      "D1 and D3: 0.1\n",
      "D1 and D4: 0.375\n",
      "D2 and D3: 0.1\n",
      "D2 and D4: 0.375\n",
      "D3 and D4: 0.3333333333333333\n",
      "\n",
      "\n",
      "Jaccard coefficient for k = 2----------\n",
      "D1 and D2: 0.3333333333333333\n",
      "D1 and D3: 0.0\n",
      "D1 and D4: 0.1111111111111111\n",
      "D2 and D3: 0.0\n",
      "D2 and D4: 0.25\n",
      "D3 and D4: 0.25\n",
      "\n",
      "\n",
      "Jaccard coefficient for k = 3----------\n",
      "D1 and D2: 0.0\n",
      "D1 and D3: 0.0\n",
      "D1 and D4: 0.0\n",
      "D2 and D3: 0.0\n",
      "D2 and D4: 0.14285714285714285\n",
      "D3 and D4: 0.18181818181818182\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_values = [1, 2, 3]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"Jaccard coefficient for k = {k}----------\")\n",
    "    kgrams = []\n",
    "    for doc, sentence in after_tokenized.items():\n",
    "        kgrams.append(set(generate_ngrams(sentence, k)))\n",
    "\n",
    "    for i in range(len(kgrams)):\n",
    "        for j in range(i + 1, len(kgrams)):\n",
    "            jc = jaccard_coefficient(kgrams[i], kgrams[j])\n",
    "            print(f\"D{i + 1} and D{j + 1}: {jc}\")\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
