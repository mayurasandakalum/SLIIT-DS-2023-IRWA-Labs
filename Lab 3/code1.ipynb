{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "doc_list = os.listdir(cwd + \"/ngram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D1': 'I am Sam',\n",
       " 'D2': 'Sam I am',\n",
       " 'D3': 'I do not like green eggs and ham',\n",
       " 'D4': 'I do not like them, Sam I am'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = {}\n",
    "for doc in doc_list:\n",
    "    with open(cwd + \"/ngram/\" + doc) as f:\n",
    "        docs[doc.split(\".\")[0]] = f.readline()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mssel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "after_tokenized = {}\n",
    "\n",
    "for doc, sentence in docs.items():\n",
    "    after_tokenized[doc] = word_tokenize(sentence)\n",
    "\n",
    "after_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def generate_ngrams(sentence, k):\n",
    "    return list(ngrams(sentence, k))\n",
    "\n",
    "k_values = [1, 2, 3]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"K-Value: {k} -------------------\")\n",
    "    for doc, tokenized_sentence in after_tokenized.items():\n",
    "        print(f\"{doc}: {generate_ngrams(tokenized_sentence, k)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coefficient(set1: set, set2: set):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 2, 3]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"Jaccard coefficient for k = {k}----------\")\n",
    "    kgrams = []\n",
    "    for doc, sentence in after_tokenized.items():\n",
    "        kgrams.append(set(generate_ngrams(sentence, k)))\n",
    "\n",
    "    for i in range(len(kgrams)):\n",
    "        for j in range(i + 1, len(kgrams)):\n",
    "            jc = jaccard_coefficient(kgrams[i], kgrams[j])\n",
    "            print(f\"D{i + 1} and D{j + 1}: {jc}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'am', 'Sam'],\n",
       " ['Sam', 'I', 'am'],\n",
       " ['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham'],\n",
       " ['I', 'do', 'not', 'like', 'them', ',', 'Sam', 'I', 'am']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "after_tokenized = []\n",
    "\n",
    "for doc, sentence in docs.items():\n",
    "    after_tokenized.append(word_tokenize(sentence))\n",
    "\n",
    "after_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k value: 1-----------------\n",
      "D1 and D2: 1.0\n",
      "D1 and D3: 0.1\n",
      "D1 and D4: 0.375\n",
      "D2 and D3: 0.1\n",
      "D2 and D4: 0.375\n",
      "D3 and D4: 0.3333333333333333\n",
      "\n",
      "\n",
      "k value: 2-----------------\n",
      "D1 and D2: 0.3333333333333333\n",
      "D1 and D3: 0.0\n",
      "D1 and D4: 0.1111111111111111\n",
      "D2 and D3: 0.0\n",
      "D2 and D4: 0.25\n",
      "D3 and D4: 0.25\n",
      "\n",
      "\n",
      "k value: 3-----------------\n",
      "D1 and D2: 0.0\n",
      "D1 and D3: 0.0\n",
      "D1 and D4: 0.0\n",
      "D2 and D3: 0.0\n",
      "D2 and D4: 0.14285714285714285\n",
      "D3 and D4: 0.18181818181818182\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def calculate_jaccard_coefficient(set1: set, set2: set):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "k_values = [1, 2, 3]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"k value: {k}-----------------\")\n",
    "    kgrams = []\n",
    "\n",
    "    for sentence in after_tokenized:\n",
    "        kgrams.append(set(ngrams(sentence, k)))\n",
    "\n",
    "    for i in range(len(kgrams)):\n",
    "        for j in range(i + 1, len(kgrams)):\n",
    "            jc = calculate_jaccard_coefficient(kgrams[i], kgrams[j])\n",
    "            print(f\"D{i + 1} and D{j + 1}: {jc}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"python\"\n",
    "s2 = \"pythonly\"\n",
    "\n",
    "ed = nltk.edit_distance(s1, s2)\n",
    "ed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
